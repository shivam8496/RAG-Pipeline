
# Hybrid Chatbot Enhancements (`hybrid_chat.py`)

This document details the significant improvements made to the `hybrid_chat.py` script, transforming it into a more efficient, scalable, and robust asynchronous RAG (Retrieval-Augmented Generation) chatbot.

---

## 1. Smart Gatekeeper (Conditional Database Access) üß†üö™

One of the most impactful changes is the implementation of a "gatekeeper" mechanism using an initial LLM call. This acts as a smart filter to decide *if* querying the external databases (Pinecone and Neo4j) is necessary.

### How it Works

1.  **Prompt Creation (`create_gatekeeper_prompt`):** Before fetching any data, this function creates a specialized prompt. It includes the user's latest query and the last 4 messages of the chat history.
2.  **LLM Call (`call_gatekeeper_llm`):** This prompt is sent to an LLM (`gpt-4o-mini`) with instructions to respond *only* in a specific JSON format. The `response_format={"type": "json_object"}` parameter ensures the output is always valid JSON.
3.  **JSON Decision:** The LLM analyzes the query and history snippet and returns a JSON object containing:
    * `content`: The answer, if it can be fully determined from the history, otherwise `null`.
    * `continue`: `true` if new data needs to be fetched, `false` if `content` has the full answer.
    * `top_k`: How many results the user asked for (e.g., "top 3"), defaulting to `5`.
    * `depth`: The inferred graph search depth (1-5), defaulting to `1`.
4.  **Conditional Logic (`interactive_chat`):** The main chat loop checks the `continue` value in the returned JSON.
    * If `false`, the script skips the expensive database calls and directly uses the `content` from the JSON as the answer.
    * If `true`, the script proceeds to fetch data from Pinecone and Neo4j.

````markdown
# In interactive_chat():
gatekeeper_prompt = create_gatekeeper_prompt(conversation_memory, query)
ans = await call_gatekeeper_llm(gatekeeper_prompt)
print(f"DEBUG: Gatekeeper decision: {ans}")

if ans["continue"]:
    print("Hit dataset...")
    # ... fetch from Pinecone/Neo4j ...
    # ... call main RAG LLM ...
else:
    print("Dataset skipped...")
    answer = ans["content"]
````

### Benefits

  * **Reduced Latency:** Simple interactions (like "thanks" or "what did you say?") get near-instant responses.
  * **Lower Costs:** Avoids unnecessary API calls to OpenAI Embeddings, Pinecone, Neo4j, and the main RAG LLM for simple queries.
  * **Reduced Database Load:** Fewer queries hit Pinecone and Neo4j.
  * **Improved User Experience:** Makes the chatbot feel more responsive for conversational turns that don't require new knowledge retrieval.

-----

## 2\. Asynchronous Execution & Parallelization ‚ö°

The entire script was refactored to use Python's `asyncio` library, enabling concurrent execution of I/O-bound tasks (like network calls).

### How it Works

1.  **Async Functions (`async def`):** Functions performing network I/O (`embed_text`, `pinecone_query`, `fetch_graph_context`, `call_chat`, `call_gatekeeper_llm`) and the main loop (`interactive_chat`) are now defined using `async def`.
2.  **Async Clients:** The OpenAI and Neo4j libraries are initialized using their async versions (`AsyncOpenAI`, `AsyncGraphDatabase`).
3.  **`await` Keyword:** Calls to async functions are now preceded by `await`, which allows the program to pause that specific task and work on others while waiting for the I/O operation (e.g., API response) to complete.
4.  **Parallel Gatekeeper & Embedding (`asyncio.gather`):** A key optimization occurs when a new query requires fetching data. The script runs the **gatekeeper LLM call** and the **text embedding call** *simultaneously*.
    ```python
    # In interactive_chat():
    task_gatekeeper = asyncio.create_task(call_gatekeeper_llm(gatekeeper_prompt))
    task_embed = asyncio.create_task(embed_text(query))
    ans, query_vector = await asyncio.gather(task_gatekeeper, task_embed) # Runs both concurrently
    ```
    This saves significant time because the embedding needed for Pinecone is calculated *while* the gatekeeper LLM is deciding what to do next.
5.  **Handling Synchronous Code (`asyncio.to_thread`):** Some operations remain synchronous (like the Pinecone client's `.query()` method and Python's built-in `input()`). To prevent these from blocking the async event loop, they are wrapped in `await asyncio.to_thread(...)`, which runs them in a separate background thread.

### Benefits

  * **Reduced Perceived Latency:** By running the initial embedding and gatekeeper calls in parallel, the user waits less time before the database queries begin (if needed).
  * **Improved Responsiveness:** The application remains responsive even while waiting for network requests, especially important in a server environment handling multiple users.
  * **Efficient Resource Usage:** `asyncio` is generally more efficient than multi-threading for I/O-bound tasks.

-----

## 3\. Optimized & Dynamic Neo4j Query ‚öôÔ∏è

The method for fetching graph context (`fetch_graph_context`) was completely overhauled from the original N+1 query approach.

### Old Approach (Slow)

```python
# Simplified old logic
facts = []
for nid in node_ids: # Loop in Python
    # Runs ONE Cypher query per node ID (N+1 problem)
    recs = session.run("MATCH (n {id:$nid})-[]-(m) RETURN m LIMIT 10", nid=nid)
    # ... append facts ...
return facts
```

### New Approach (Fast & Flexible)

```python
# Simplified new logic
async def fetch_graph_context(node_ids: tuple, neighborhood_depth=1):
    # ... sanitize depth ...
    # Dynamically build query string based on depth
    path_match_query = f"MATCH p = (n)-[*1..{depth}]-(m:Entity)"
    # ... build rest of query ...
    q = f"""
    UNWIND $node_ids AS nid      # Process all IDs inside Cypher
    MATCH (n:Entity {{id: nid}})
    CALL (n) {{                 # Subquery executes FOR EACH node 'n'
       {path_match_query}       # Use dynamic path match
       WHERE n <> m
       RETURN ...               # Return path length, synthetic rel, labels, etc.
       ORDER BY length(p) ASC   # Prioritize closer nodes
       LIMIT 10                 # Apply LIMIT *per node* inside the subquery
    }}
    RETURN nid AS source, ...    # Collect results from all subquery runs
    """
    async with driver.session() as session:
        # Run ONE Cypher query for ALL node IDs
        recs = await session.run(q, node_ids=list(node_ids)) # Pass tuple as list
        # ... async iterate and append facts ...
    return facts

```

### Key Changes & Benefits

  * **Eliminated N+1 Problem:** Uses `UNWIND` to process all `node_ids` in a single query, drastically reducing database round-trips and improving performance by orders of magnitude. üöÄ
  * **Dynamic Depth:** Uses Python f-strings (safely, after sanitizing `neighborhood_depth`) to create queries like `MATCH (n)-[*1..3]-(m:Entity)`, allowing variable search depth based on the gatekeeper's decision.
  * **`LIMIT` Per Node:** Uses `CALL (n) { ... LIMIT 10 }` subquery structure. This ensures the `LIMIT 10` applies to *each* starting node, rather than limiting the total results, providing richer context.
  * **Robust Type Handling:** Since variable paths (`[*1..{depth}]`) return paths, not single relationships, `type(r)` is no longer valid. The query now calculates path length (`length(p)`) and generates:
      * A synthetic relationship type (e.g., `RELATED_AT_DEPTH_2`).
      * The specific node `type` (e.g., "City") by extracting it from the node's labels (`[l IN labels(m) WHERE l <> 'Entity'][0] AS type`).
  * **Modern Syntax:** Uses the recommended `CALL (n) { ... }` syntax instead of the deprecated `CALL { WITH n ... }`.
  * **Ordering:** Adds `ORDER BY path_len ASC` before the `LIMIT` to prioritize fetching the closest related nodes first when depth \> 1.

-----

## 4\. Asynchronous Caching (`async-lru`) üíæ

To further reduce latency and API costs for repeated operations, asynchronous caching was added using the `async-lru` library.

### How it Works

1.  **Installation:** `pip install async-lru`
2.  **Decorator:** The `@alru_cache(maxsize=...)` decorator is applied to `async def` functions (`embed_text`, `pinecone_query`, `fetch_graph_context`).
3.  **Mechanism:** When a decorated async function is called, `alru_cache` checks if a call with the *exact same arguments* has been made recently.
      * If yes (cache hit), it returns the stored result instantly without executing the function body or making network calls. ‚úÖ
      * If no (cache miss), it executes the function, makes the network calls, stores the result, and then returns it. üê¢‚û°Ô∏èüêá
4.  **Hashable Arguments:** Caching requires function arguments to be "hashable" (immutable). Since Python lists are not hashable, `fetch_graph_context` was modified to accept `node_ids: tuple`. The `interactive_chat` loop converts the list of `match_ids` to a `tuple` before calling `fetch_graph_context`.

<!-- end list -->

```python
# Example decorator usage
@alru_cache(maxsize=128)
async def embed_text(text: str) -> List[float]:
    # ... function body ...

# Example tuple conversion before calling cached function
match_ids_tuple = tuple(m["id"] for m in matches)
graph_facts = await fetch_graph_context(match_ids_tuple, ...)
```

### Benefits

  * **Faster Responses:** Repeated requests for the same embedding, Pinecone search, or Neo4j context are served almost instantly from memory.
  * **Reduced API Costs:** Fewer calls are made to OpenAI Embeddings, Pinecone, and Neo4j for identical inputs.

-----

## 5\. Enhanced Prompt Engineering ‚úçÔ∏è

The prompts sent to the LLMs were refined for better clarity, robustness, and reasoning quality.

### Gatekeeper Prompt (`create_gatekeeper_prompt`)

  * **Clear Rules:** Explicitly defines the logic for setting `content`, `continue`, `top_k`, and `depth`.
  * **JSON Enforcement:** Instructs the LLM to *only* output JSON and relies on the OpenAI API's `response_format={"type": "json_object"}` to guarantee valid JSON, eliminating fragile regex parsing.
  * **Context Snippet:** Provides the last 4 messages (`history_str`) as context for the decision.
  * **Examples:** Includes clear examples for different scenarios.

### Main RAG Prompt (`build_rag_prompt`)

  * **Chain-of-Thought (CoT):** Instructions were added to the system prompt asking the LLM to perform a step-by-step reasoning process within `<thinking>` tags *before* generating the final answer. This encourages more logical, grounded responses.
    ```
    # Inside system prompt:
    "Before generating the final answer, perform a step-by-step analysis within <thinking> tags:"
    "1. **Identify User Intent:** ..."
    "2. **Extract Key Entities:** ..."
    "3. **Analyze Relationships:** ..."
    "4. **Synthesize Plan:** ..."
    "After your thinking process, provide the final answer... DO NOT include the <thinking> tags..."
    ```
  * **Full History Context:** Correctly passes the *entire* (or summarized, if long) `chat_history` list to the LLM using the `*chat_history` expansion, ensuring the model has full conversational memory.
  * **Clear Context Tagging:** Uses distinct XML tags like `<semantic_matches>` and `<graph_facts>` to clearly delineate the different sources of information for the LLM.
  * **Summarized History Handling:** Includes logic to integrate a summary message (`<summary_of_omitted_conversation>`) when the "head and tail" history truncation is used.

### Benefits

  * **Reliable Gatekeeping:** Consistent JSON output makes the gatekeeper logic robust.
  * **Improved RAG Quality:** CoT reasoning can lead to more accurate and well-supported answers. Full history ensures better conversational coherence.
  * **Transparency:** CoT makes the LLM's reasoning process (if logged or viewed internally) more transparent.

-----

## 6\. Overall Code Flow üó∫Ô∏è

The execution logic within `interactive_chat` now follows this refined, asynchronous flow:

1.  **Get Input:** Wait for user query (using `asyncio.to_thread`).
2.  **Parallel Start:** Concurrently:
      * Start the **Gatekeeper LLM Call** (`call_gatekeeper_llm`).
      * Start the **Text Embedding** (`embed_text`).
3.  **Wait & Decide:** `await asyncio.gather` waits for both parallel tasks to finish. Get the gatekeeper's JSON decision (`ans`) and the `query_vector`.
4.  **Branch:** Check `ans["continue"]`:
      * **If `false` (Dataset Skipped):**
          * Set `answer` directly from `ans["content"]`.
      * **If `true` (Hit Dataset):**
          * **Query Pinecone:** Use the pre-computed `query_vector` (via `asyncio.to_thread`).
          * **Query Neo4j:** Use Pinecone `match_ids` and `ans["depth"]` (`await fetch_graph_context`).
          * **Manage History:** Check `len(conversation_memory)`. If \> threshold (e.g., 25):
              * Split history into head, middle, tail.
              * `await summarize_history_chunk(middle)` using an LLM call.
              * Construct `context_history` = head + summary\_message + tail.
          * Else (history is short):
              * `context_history` = `conversation_memory`.
          * **Build RAG Prompt:** Call `build_rag_prompt` with the query, appropriate `context_history`, retrieved `matches`, and `graph_facts`.
          * **Call Main LLM:** `await call_chat` with the RAG prompt to get the final `answer`.
5.  **Output:** Print the `answer` to the user.
6.  **Update Memory:** Append the user's `query` and the final `answer` to the `conversation_memory` list.
7.  **Loop:** Go back to step 1.

This flow prioritizes speed through parallelization and caching, reduces unnecessary work via the gatekeeper, handles long conversations gracefully with summarization, and aims for high-quality answers using CoT and full context.

```
```